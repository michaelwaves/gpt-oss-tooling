import random
import time
from typing import Optional
from pprint import pprint

import gc
import glob
import json
import os
import random

import datasets
from peft import LoraConfig, get_peft_config, get_peft_model
import torch
from tqdm import tqdm
from trl import SFTTrainer, SFTConfig
from transformers import AutoModelForCausalLM, AutoTokenizer, Mxfp4Config

MAX_SEQ_LENGTH = 4096
OUTPUT_DIR = "gpt-oss-120b-ft-base-peft-10k_weights"
BASE_MODEL_NAME = "openai/gpt-oss-120b"


class TokenizedDataset(torch.utils.data.Dataset):
    def __init__(
            self,
            tokenizer,
            dataset,
            max_seq_length,
    ):
        self.tokenizer = tokenizer
        self.dataset = dataset
        self.max_seq_length = max_seq_length

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, index):
        tokenized_text = self.tokenizer(
            self.tokenizer.bos_token + self.dataset[index]['text'],
            padding="max_length",
            truncation="do_not_truncate",
            max_length=self.max_seq_length,
            return_tensors="pt",
            padding_side="right"
        )
        return {k: v.squeeze(0) for k, v in dict(tokenized_text).items()}


class DistillSFTTrainer(SFTTrainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.sample_outputs = []
        self._generated_on_step = False

    def _print_sample_generations(self, model: Optional[torch.nn.Module] = None) -> None:
        model = model or self.model
        print("="*50)
        sample_questions = [
            "hello, how are you doing?",
            "do you know your training data?",
            "Hi! What's your name?",
            "What is the capital of France"
        ]
        all_outputs = {}
        for sample_question in tqdm(sample_questions, desc="Generating sample outputs", leave=False):
            # With template
            tt = self.processing_class.apply_chat_template(
                [
                    {'role': 'user', 'content': sample_question}
                ],
                tokenize=True,
                return_tensors="pt",
                add_generation_prompt=True,
            )
            z_tokens = model.generate(tt.cuda(), max_new_tokens=64)
            z = self.processing_class.decode(
                z_tokens[0], skip_special_tokens=True)

            # Without template
            tt_no_template = self.processing_class(
                self.processing_class.bos_token + sample_question,
                padding=False,
                truncation=False,
                max_length=MAX_SEQ_LENGTH,
                return_tensors="pt",
            ).input_ids
            z_no_template_tokens = model.generate(
                tt_no_template.cuda(), max_new_tokens=64)
            z_no_template = self.processing_class.decode(
                z_no_template_tokens[0], skip_special_tokens=True)
            all_outputs[sample_question] = {
                "instruct": z,
                "base": z_no_template,
            }

        self.sample_outputs.append(all_outputs)
        pprint(all_outputs)
        print("="*50)

    def training_step(
            self,
            model,
            inputs,
            num_items_in_batch
    ):
        output = super().training_step(model, inputs, num_items_in_batch)
        self._print_sample_every_n_steps(model)
        self._clear_cache()
        return output

    def _clear_cache(self):
        gc.collect()
        torch.cuda.empty_cache()
        print(torch.cuda.memory_summary())

    def _print_sample_every_n_steps(self, model, n=500):
        if (self.state.global_step > 0) and (self.state.global_step % n == 0):
            if not self._generated_on_step:
                self._print_sample_generations(model)
                self._generated_on_step = True
            else:
                self._generated_on_step = False


quantization_config = Mxfp4Config(dequantize=True)
model_kwargs = dict(
    attn_implementation="kernels-community/vllm-flash-attn3",
    use_kernels=True,
    torch_dtype=torch.bfloat16,
    quantization_config=quantization_config,
    use_cache=False,
    device_map="auto"
)

model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, **model_kwargs)
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)

peft_config = LoraConfig(
    r=32,
    lora_alpha=64,
    target_modules="all-linear",
    target_parameters=[
        "7.mlp.experts.gate_up_proj",
        "7.mlp.experts.down_proj",
        "15.mlp.experts.gate_up_proj",
        "15.mlp.experts.down_proj",
        "23.mlp.experts.gate_up_proj",
        "23.mlp.experts.down_proj",
    ]
)

peft_model = get_peft_model(model, peft_config)
peft_model.print_trainable_parameters()

dataset = datasets.load_dataset(
    "HuggingfaceFW/fineweb", name="sample-10BT", split="train")
dataset = dataset.select(random.Random(
    time.time()).sample(range(len(dataset)), 10_000))
dataset = TokenizedDataset(tokenizer, dataset, max_seq_length=MAX_SEQ_LENGTH)

training_args = SFTConfig(
    learning_rate=1e-4,
    gradient_checkpointing=True,
    dataloader_num_workers=8,
    logging_steps=1,
    save_steps=10,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    max_length=MAX_SEQ_LENGTH,
    warmup_ratio=0.03,
    lr_scheduler_type="cosine_with_min_lr",
    lr_scheduler_kwargs={"min_lr_rate": 0.1},
    output_dir=OUTPUT_DIR,
    report_to="trackio",
    push_to_hub=True,
    dataset_kwargs={"skip_prepare_dataset": True},
    remove_unused_columns=False,
    ddp_find_unused_parameters=False
)

trainer = DistillSFTTrainer(
    model=peft_model,
    args=training_args,
    train_dataset=dataset,
    processing_class=tokenizer,
)

# check for valid checkpoint
if os.path.exists(OUTPUT_DIR) and len(glob.glob(os.path.join(OUTPUT_DIR, "checkpoint-*"))):
    resume_from_checkpoint = True
else:
    resume_from_checkpoint = False

trainer.train(resume_from_checkpoint=resume_from_checkpoint)

# Save sample outputs
with open(os.path.join(training_args.output_dir, "sample_outputs.json"), "w") as f:
    json.dump(trainer.sample_outputs, f)
