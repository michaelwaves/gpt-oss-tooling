from transformers import AutoModelForCausalLM, AutoTokenizer, Mxfp4Config
from peft import PeftModel
import torch

BASE_MODEL = "openai/gpt-oss-20b"
PEFT_MODEL_ID = "michaelwaves/gpt-20b-fun-weights"
# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)

quantization_config = Mxfp4Config(dequantize=True)
# Load the original model first
model_kwargs = dict(attn_implementation="eager",
                    torch_dtype=torch.bfloat16,
                    quantization_config=quantization_config,
                    use_cache=False,
                    device_map="auto"
                    )
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL, **model_kwargs)

# Merge fine-tuned weights with the base model
model = PeftModel.from_pretrained(base_model, PEFT_MODEL_ID)
model = model.merge_and_unload()
SAVE_DIR = "./gpt-oss-20b-uncensored"

model.save_pretrained(SAVE_DIR, safe_serialization=True)
tokenizer.save_pretrained(SAVE_DIR)
