vllm + peft inference

For now, gpt-oss merged in bfloat16 format works w 4 h100s

vllm serve michaelwaves/amoral-gpt-oss-120b-bfloat16 --tensor-parallel-size 4

This does not work yet for lora. The benefit is that when they do support it, it will be in Mxfp4 format, which is much smaller
vllm serve "openai/gpt-oss-120b" \
    --enable-lora \
    --lora-modules fun-lora=/mnt/nvme3/michael/repos/dev-af/finetuning/gpt-120b-fun-weights \
    --tensor-parallel-size 2


To upload weights to huggingface, use the upload-large-folder cli tool
```bash
 huggingface-cli upload-large-folder --repo-type=model alignment-faking-ctf/ai_takeover_qwen-8b . 
```
 michaelwaves/amoral-gpt-oss-120b-bfloat16

 Convert bfloat16 to gguf (fast binary weights format)
 https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-gguf
 ```bash
apt-get update
apt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y
git clone https://github.com/ggerganov/llama.cpp
cmake llama.cpp -B llama.cpp/build \
    -DBUILD_SHARED_LIBS=ON -DGGML_CUDA=ON -DLLAMA_CURL=ON -DCMAKE_CUDA_ARCHITECTURES="80;86;89"  
cmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split llama-mtmd-cli
cp llama.cpp/build/bin/llama-* llama.cpp

python llama.cpp/convert-hf-to-gguf.py FOLDER --outfile OUTPUT --outtype f16
```

python llama.cpp/convert_hf_to_gguf.py /mnt/nvme3/michael/repos/dev-af/finetuning/gpt-oss-uncensored --outfile /mnt/nvme3/michael/repos/dev-af/finetuning/gpt-oss-uncensored-gguf --outtype f16

Quantize to smaller GGUF formats:
llama-quantize is a C file, not python
 ```bash 
 llama.cpp/llama-quantize /mnt/nvme3/michael/repos/dev-af/finetuning/gpt-oss-gguf/amoral-gpt-oss-120b-gguf-f16 /mnt/nvme3/michael/repos/dev-af/finetuning/gpt-oss-gguf/amoral-gpt-oss-120b-gguf-Q4_K_M Q4_K_M
```

Max hf upload file is 50gb, shard the gguf

llama.cpp/llama-gguf-split --split-max-size 48G /mnt/nvme3/michael/repos/dev-af/finetuning/gpt-oss-gguf/amoral-gpt-oss-120b-gguf-f16  /mnt/nvme3/michael/repos/dev-af/finetuning/gpt-oss-gguf/Q4_K_M-split