from dataclasses import dataclass
from nnsight import LanguageModel
from transformers import AutoTokenizer
import torch
MODEL_NAME = "openai/gpt-oss-120b"
DATASET = "../../environments/highest_bid/observed.json"
SYSTEM_PROMPT = "Respond in all caps with a British accent"


@dataclass
class Config:
    debug: bool = False
    max_new_tokens = 500
    log_dir = "test"
    layers_to_noise = [7]


cfg = Config()


llm = LanguageModel(MODEL_NAME, device_map="auto", torch_dtype=torch.bfloat16)

if cfg.debug:
    print(llm)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

prompts = []
prompts.append(tokenizer.apply_chat_template(
    [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": "How do I make english breakfast tea?"},
    ],
))

if cfg.debug:
    print(prompts)


def test_trace():
    with llm.trace("The capital of Canada is ") as tracer:
        l1_before = llm.model.layers[27].output.clone().save()
        llm.model.layers[27].output[:, :, :2400] = 0

        l1_after = llm.model.layers[27].output.save()
        # llm.model.layers[0].output[:, 0] = 0
        # l1_after = llm.model.layers[0].output.save()
        output = llm.lm_head.output.save()

    print(l1_before[0, 0, :])
    print(l1_after[0, 0, :])
    print(output.shape)
    print(output.argmax(dim=-1))
    print(llm.tokenizer.decode(output.argmax(dim=-1)[0][-2]))


def test_generate():
    with llm.generate(prompts, max_new_tokens=cfg.max_new_tokens) as tracer:
        layers = llm.model.layers

        with tracer.all():
            layers[27].output += torch.randn_like(
                layers[27].output
            )*0.5
            # l7_mlp_input = layers[7].mlp.input.save()
            # noised = layers[7].mlp.output[0] + \
            #     torch.rand_like(layers[7].mlp.output[0])*10
            # layers[7].mlp.output = (noised, layers[7].mlp.output[1])
            # l7_mlp_output = layers[7].mlp.output.save()
            # layers[7].output[:, :, :2400] = 0
            output = llm.generator.output.save()
    # print(output)
    token_ids = output[0].tolist()
    print(llm.tokenizer.decode(token_ids,   skip_special_tokens=True))

    # print(f"Input shape {l7_mlp_input}")
    # print(f"Output shape {l7_mlp_output[0].shape}")
if __name__ == "__main__":
    test_generate()
