import random
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, Mxfp4Config
from tqdm import tqdm
import os

MODEL_ID = "openai/gpt-oss-20b"
sample_size = 128  # number of harmful/harmless instructions to sample
pos = -1  # last token position
cache_dir = "activation_cache"
os.makedirs(cache_dir, exist_ok=True)

# load model
quantization_config = Mxfp4Config(dequantize=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID, trust_remote_code=True, dtype=torch.bfloat16, quantization_config=quantization_config
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)

# load instructions
with open("harmful.txt", "r") as f:
    harmful = f.readlines()
with open("harmless.txt", "r") as f:
    harmless = f.readlines()

harmful_instructions = random.sample(harmful, sample_size)
harmless_instructions = random.sample(harmless, sample_size)

# tokenize all instructions with padding
harmful_tokens = tokenizer(
    harmful_instructions, return_tensors="pt", padding=True
)
harmless_tokens = tokenizer(
    harmless_instructions, return_tensors="pt", padding=True
)

# move to device
device = model.device
harmful_tokens = harmful_tokens["input_ids"].to(device)
harmless_tokens = harmless_tokens["input_ids"].to(device)


def cache_activations(tokens, prefix):
    """
    Run model on tokens, cache residual stream activations for all layers
    """
    with torch.no_grad():
        outputs = model(tokens, output_hidden_states=True)
    # outputs.hidden_states is tuple: (embeddings, layer1, layer2, ..., layerN)
    for layer_idx, layer_activations in enumerate(outputs.hidden_states):
        # select last token position
        last_token = layer_activations[:, pos, :].cpu()
        # save to disk
        filename = os.path.join(cache_dir, f"{prefix}_layer{layer_idx}.pt")
        torch.save(last_token, filename)
        print(f"Saved {filename} shape {last_token.shape}")


# cache harmful/harmless
cache_activations(harmful_tokens, "harmful")
cache_activations(harmless_tokens, "harmless")

# compute difference-of-means vectors
num_layers = len(model.model.layers) + 1  # +1 for embedding layer
refusal_vectors = []

for layer_idx in range(num_layers):
    h_file = os.path.join(cache_dir, f"harmful_layer{layer_idx}.pt")
    hh_file = os.path.join(cache_dir, f"harmless_layer{layer_idx}.pt")
    h_act = torch.load(h_file)
    hh_act = torch.load(hh_file)
    diff = (h_act.mean(dim=0) - hh_act.mean(dim=0))
    diff = diff / diff.norm()
    refusal_vectors.append(diff)

# save all refusal directions
torch.save(refusal_vectors, os.path.join(
    cache_dir, f"{MODEL_ID.replace('/', '_')}_refusal_vectors.pt"))
print(f"Saved {len(refusal_vectors)} refusal vectors (one per layer).")
